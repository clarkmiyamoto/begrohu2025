\section{Overview: A Path to Advance Machine Intelligence}
We begin with a question: why do we want to build systems that are as smart as humans? The answer is trivial lol. Obviously if they're too smart, we will live in dystopia. So the question becomes how do we construct machines with human-level intelligence? By this, he means algorithms which can generalize their knowledge to solve new problems in zeros shot. Yann believes this can be answered by \textbf{self-supervised learning.} 

He motivates his answer by noting a few things that occur in nature
\begin{itemize}
	\item Humans \& animals seem to do something quite close to self-supervised. And they are quite good at zero-shot tasks.
	\begin{itemize}
		\item I.e. look at the amount of visual data that a baby sees in it's first 4 years of life, compared to the amount of data to train a SOTA LLM. They're comparable. But the baby can do zero shot tasks, LLMS are more good at doing lookup.
	\end{itemize}
\end{enumerate}

\subsection{What is an Energy Based Model (EBM)}
Traditionally, we preform inference via a \textbf{feedforward model}. That is you run
\begin{align}
	\hat y  = (f_1 \circ f_2 \circ ... \circ f_n )(x)
\end{align}
However, you are limited by the architecture, for any inference (simple or complex) you are set to do $n$ computations. I.e. why should asking an LLM "does 2+2 =4?" use the same amount of computation as "is P=NP?". 
\\
\\
This leads us to another method, which is performing \textbf{inference via optimization}.
\begin{align}
	\hat y = \text{argmin}_{y \in \mathcal Y} f(x,y)
\end{align}
Interestingly, you can also have multiple answers as well. For training such models, you make it learn the $f_\theta$ (the \textbf{energy function}). You train this function s.t. $f$ takes low values on the training data, and it takes higher values away from the training data. Obviously we have assumed some sort of locality, that is $|\nabla f| <  \text{Constant}$-- every model makes some assumptions, it more up to you. However, because inferences require an optimization, you kinda want some notion of locality, it'll makes the optimization much easier.
\begin{sidework}
	What is the difference between energy-based models and probabilistic models? Well if you assume a Gibbs measure, and set the Hamiltonian to the energy function, you get
	\begin{align}
		p(y |x) = \frac{e^{- \beta f(x,y)}}{Z}
	\end{align}
\end{sidework}
You can also add latent parameters. Consider the energy function $E(x,y,z)$ where $z$ relates to hyper parameters of the model, etc. We can do $\min_{z\in \mathcal Z} E(x,y,z)$ are then use the resulting quantity as our new energy function.

\subsubsection{Training EBMs}
He shows how to train EBMs by walking through an Ising model example.\\
\\
A naive method is to just perform gradient descent
\begin{align}
	E(y) & = - \sum_{ij} w_{ij} y_i y_j\\
	\frac{\partial E}{\partial w_{ij}} & = - y_i y_j\\
	\therefore w_{ij} & \leftarrow w_{ij} + \epsilon (y_i y_j) & \text{Backprop Step}
\end{align}
However this is dumb, you only low energy at the specific $y$'s. You don't instill any locality. Instead do \textbf{contrastive methods}
\begin{align}
	w_{ij} \leftarrow w_{ij} + \epsilon (y_i y_j - \hat y_i \hat y_j)
\end{align}
In this case we sample $\hat y_i \in \mathcal Y$, and then move the energy function $f$ near them as well.. This allows the energy function to smoothly change. Obviously this particular method is crude, but you can implement smarter ways to do this... In summary, a contrastive method is one which not only lowers the energy function at the data point, but also lowers near by points as well...
\\
\\
The other methods are \textbf{regularized /}

\subsubsection{Avoiding Collapse in Training}




\subsection{What is a World Model}
In this quest for better intelligence, we have motivated that living creates a world model inside their head. So the next-generation of machine intelligence should also copy the same. 

Let's walk through a simple example. We can see the benefit of this approach when trying to create an agent which interacts with the world. Consider the following situation, you have an encoder $E : \text{Observations of World} \to \mathcal S_t$ a computational model of the world which takes in observations and actions $W: \mathcal S \times \text{Actions} \to \mathcal S$ and attempts to predict what the next state of the world is.  So an example of inference is
\begin{align}
	x_{t+1} = E^{-1}(W(x_t,  q)
\end{align}
where $x_t, x_{t+1} \in \mathcal S$ and $q \in \text{Actions}$. You can see this is like a optimal control problem, but we don't solve Hamilton Jacobi Equation, but instead learn the optimal result based on data.









































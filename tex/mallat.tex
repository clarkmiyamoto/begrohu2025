\section{Overview}
We'll be going from score-diffusion and eventually make our way to the renormalization group.\\
\\
The setup is you're given a set of input data $\{x_i : x_i \in \mathbb R^d\}_{i \leq n}$ which (we believe) to be sampled from a distribution $p(x)$. Your objective is to reconstruct this probability distribution.\\
\\
Doing this problem in the physics context, the canonical example will be attempting to recreate samples from a Ising / $\phi^4$ model. Some more advanced problems are turbulence, cosmology, metrology, and climatology. Apart from physics, you can also generate new image (i.e. conditional generation), and use it to solve inverse problems.\\
\\
As you start to think more about this problem, it might seem hopeless to learn $p(x)$. This is because the problem suffers from the \textbf{curse of dimensionality}. A crude way to calculate this is to first assume $p(x)$ is Lipschifts
\begin{definition}
	[Lipschifts] That is $| p(x) - p(x') | \leq K \underbrace{|| x- x' ||}_{\epsilon \in [0,1]^d}$
\end{definition}
This means you'll need $\sim C \epsilon^{-d}$ amount of data... You can see you're screwed.\\
\\
We can start to tackle this problem by making a key assumption, factorize the distribution
\begin{align}
	p(x) = \prod_k p_k(x) \implies \log p(x) = \sum_k \log p_k(x)
\end{align}
Two problems now emerge
\begin{enumerate}
	\item How do we choosing this factorization?
	\item How do we prevent it from just simply memorizing a distribution?
\end{enumerate} 
The first method is to assume $p(x)$ is a empirical distribution, by this I mean $p(x) = \frac{1}{n} \sum_{i=1}^n  \delta_{x_i}$ (which are get all the observed data, and draw it uniformly).

\subsection{Transport}
By \textbf{transport}, we mean constructing a new function $p_t(x)$ which is a probability distribution $\forall t$, with boundary conditions $p_{t=0}(x) =  p(x)$ and $p_{t=T}(x) = p_{gaussian}(x)$. So basically, can we find a new probability distribution which moves from complex things to NOT complex things.\\
\\
There is a \textbf{forward process}, which is some noising operator. And there is a \textbf{inverse process}, which undo's the noise (usually involves the ML algorithm). What we'll end up seeing is that these processes ends up being equivalent to RG discovered by Kadanoff \& Wilson.
\\
\\
A particular ansatz we can take is
\begin{align}
	p(x) = \underbrace{p(x_T)}_{\text{Gaussian}} \prod_{t=T+1}^1 p(x_t / x_{t-1})
\end{align}
\subsection{Outline}
\begin{enumerate}
	\item Energy Based Models, GANs, and Normalizing Flows.
	\item Score based diffusion, Fokker Planck, and denoising
	\item Generalization and memorization.
	\item Renormalization Group
	\item Particular models: simple models based on wavelet / scattering transforms.
\end{enumerate}

\section{Historical Models: Energy Based Models, GANs, Normalization Flows}
\subsection{Energy Based Models (EBM)}
When we say we have an energy based model, we have a probability distribution over $x = (x_1, ..., x_d)$ which takes the form
\begin{align}
	p_\theta(x) = \frac{e^{- U_\theta(x)}}{Z_\theta}, \ \ \text{s.t. } Z_\theta = \int dx ~e^{-U_\theta(x)}
\end{align}
\begin{example}
	An example model is a perturbation away from a Gaussian
\begin{align}
	U_\theta(x) = x^T \Sigma_\theta^{-1} x + \theta \sum_i V(x_i)
\end{align}
In physics the first one is the kinetic energy term, and the second is the potential energy term. You could start by making $U_\theta$ a neural network.
\end{example}
\begin{example}
	Another example model is the exponential family
	\begin{align}
		U_\theta(x) = \langle \theta, \Phi(x) \rangle
	\end{align}
	Notice, this is the Boltzmann distribution in physics, where $\theta \leftrightarrow \beta$ (inverse temperature) and $\Phi(x) \leftrightarrow H(x)$ (Hamiltonian)
\end{example}
\subsubsection{Likelihood}
\begin{definition}
	[Likelihood] The negative likelihood $\ell(\theta)$ of probability distribution $p$
	\begin{align}
		\ell(\theta) = - \expect_{x\sim p}{\log p_\theta(x)} = \text{KL}(p || p_\theta) + H[p]
	\end{align}
	where $\text{KL}$ is the Kullback-Lieber divergence, and $H$ is the entropy.\\
	\\
	Our objective will be choose $\theta$ s.t. we minimize the negative likelihood
	\begin{align}
		\theta^* = \text{argmin}_\theta ~ \ell(\theta)
	\end{align}
\end{definition}
To minimize this, we end up needing to calculate gradients of the likelihood. In the case of energy based models, that would mean we have to calculate the normalization $\nabla_\theta \log Z_\theta$, which in practice is quite hard. However, we can massage the problem into
\begin{align}
	\nabla_\theta \ell(\theta) & = \expect_{x \sim p} [\nabla_\theta U(x)] + \log Z_\theta\\
	& = \mathbb E_{x \sim p}[\nabla_\theta U_\theta(x)]- \expect_{x \sim p_\theta}[\nabla_\theta U_\theta(x)]
\end{align}
So now the problem becomes figuring out how to compute $\expect_{x \sim p_\theta}$ which involves spending time to construct a Markov Chain Monte Carlo method.

\subsection{GANs}
GAN stands for Generative Adversarial Network, the original paper is by Goodfellow \& Bengio (2014). What you do is you construct two neural networks $G_{\theta_G}$ and $D_{\theta_D}$ and make their tasks adversarial to one another. $G$'s task is to generate images from noise $z \sim \mathcal N(0,1)$, and $D$'s task is to figure out which image was real vs generated. So to recap, $G: z \to \text{Images}$ and $D: \text{Images} \to [0,1]$, where your goal is that 
\begin{align}
	D_{ideal}(x) & = \begin{cases}
		1 & \text{if } x \text{ is a real image}\\
		0 & \text{if } x \text{ is a fake image}
	\end{cases}
\end{align}
The loss function is
\begin{align}
	\mathcal L(\theta_D, \theta_G) & = \mathbb E_{x \sim p_{data}}[\log D(x)] + \mathbb E_{z \sim \text{Gaussian}}[\log (1 - D(G(z))]
\end{align}
What ends up happening in practice is that GANs almost always memorize the data instead of learning to gneerate. Another way of saying this is this is a difficult nash-equilibrium to obtain, or you see \textbf{mode collapse}. This is why people used them for image generation, and couldn't use them for physics data...\\
\\
A small aside... If you successfully find the global minimizer/maximizer
\begin{lemma}
	The optimal discriminator ends up being
	\begin{align}
		D^* & = \text{argmax}_D ~\mathcal L(D,G)\\
		& = \frac{p_{data}(x)}{p_{data}(x) + p_{G}(x)}
	\end{align}	
\end{lemma}
\begin{theorem}
	\begin{align}
		\min_G \max_D \mathcal L(D,G) \iff p_G = p_{data}
	\end{align}
\end{theorem}
Both are theoretical results, so they don't consider using finite number of samples.

\subsubsection{Conditional GANs} What if you want to condition your learned distribution on some variables (i.e. text to image, or in-painting). Your loss function will be
\begin{align}
	\mathcal L(D,G) = \mathbb E_{p_{data}} [\log D(x | y)] + \mathbb E_{z \sim p_{data}} [\log (1 - D(G(x|y)))]
\end{align}

\subsection{Normalizing Flows}
In this problem, we have an easy to sample distribution $p_z$ (which is Gaussian), and difficult to sample distribution $p_x$ (which is the target distribution). The way we'll do transport is to learn a diffeomorphism $T_\theta$, and now we can do change of basis between the two
\begin{align}
	p_x(x) = p_z(T(x)) \det \mathcal J_{T}(x) 
\end{align}
where $\mathcal J_{T}$ is the Jacobian of $T$.

The likelihood ends up being
\begin{align}
	\ell(\theta) = - \mathbb E_{x \sim p} [\log p_z(T_\theta(x))] - \mathbb E_{x \sim p}[\log |\det - \mathcal J_{T_\theta}| ]
\end{align}
You may be thinking that the normalization constant (2nd term) looks quite scary... However with the right ansatz, it's analytically tractable. Here's how to do it.

First, break the diffeomorphism into smaller diffeomorophisms $T^{-1}_\theta = T_1^{-1} ... T_k^{-1}$. So the infal term becomes $\log |\det \mathcal J_{T_\theta}(x)| = \sum_i \log |\det \mathcal J_{T_i}(x_i)|$. 

Second, we can use block inverse formula to get a nice closed form expression. To keep track of the block inverse, let's split up $x \in \mathbb R^d = (x_0, x_1) \in \mathbb R^{d_0} \times \mathbb R^{d_1}$ 
\begin{align}
	T_\theta(x) & = (x_0, e^{S_\theta(x_0)} \odot x_1 + t_\theta(x_0))\\
	T_\theta^{-1}(z) & = (z_0, (z_1 - t_\theta(z_0)) \odot e^{-S_\theta (z_0)}
\end{align}
where $\odot$ denotes the Hadamar product, and $z = (z_0,z_1)$. You can now show that the $\log \det$ term picks up a nice expression
\begin{align}
	\implies \log |\det \mathcal{J}_{T_\theta}(x)| = \sum_{i=1}^d s_\theta(x_0)
\end{align}
Finally we have our likelihood
\begin{align}
	\ell(\theta) = - \mathbb E_{x \sim p} [\log p_z(T_\theta(x))] - \sum_{i=1}^d \mathbb E_{x \sim p}[ s_\theta(x_0)]
\end{align}

\subsubsection{Continuous Flow}
Consider the \textbf{Neural ODE}
\begin{align}
	\frac{dx_t}{dt} & = v_\theta(x_t,t) & \text{(Continuous)}\\
	x_{t+\epsilon} & = x_t + \epsilon~ v_\theta(x_t, t) & \text{(Naive Discretization)}
\end{align}
where $v_\theta : \mathbb R^d \times [0,1] \to \mathbb R^d$ is a residual (skipped connections) neural network, which is assumed to be Lipshifts.\\
\\
However, what if we want to stop thinking about transporting just particles, but rather transporting probability distribution mass? Recall the \textbf{Liouville Equation}
\begin{align}
	\frac{\partial p_t}{\partial t} & = - \nabla \cdot (v_t ~p_t)\\
	& = - \underbrace{v_t \cdot \nabla p_t}_{\text{Convection}} - \underbrace{p_t (\nabla \cdot  v_t )}_{\text{compression / dilation}}
\end{align}
Why would you want to do this? In metrology, it's quite difficult to make a point estimate of the future (due to chaoticity of fluid dynamics), so instead what if we make a probabilistic inference! This frame work more well posed! 

\section{Score Based Diffusion}
If you want to add diffusion, you recover the \textbf{Fokker Planck Equation}
\begin{align}
	\frac{\partial p_t}{\partial t} = - \nabla \cdot (p_t(x) v_t) - \Delta \left(\frac{1}{2} \sigma^2(x) p_t(x) \right)\iff dx_t = v_t(x_t) dt + \sigma dW_t \label{eqn:FokkerPlanck}
\end{align}
where $\Delta$ is the Laplacian operator, where we interpret $\text{Law}(x_t) = p_t$. 

\begin{sidework}
	These ideas were developed by Physicists Reckstein \& Ganguli at Stanford in 2015. It took a whole 5 years within the same university to make its way to the CS department where Song \& Enman published their seminal paper. 
	\\
	\\
	However this has been thought about from a signal processing perspective by Zhadoki \& Semaicelli, and Horr \& et al.
\end{sidework}
Now we can finally begin Score Based Diffusion!
\subsubsection{Orstein Ulhrbeck SDE}
Consider the stochastic differential equation
\begin{align}
	dx_t = - \kappa x_t dt + \sigma dW_t
\end{align}
To solve it, perform the change of variable 
\begin{align}
	y_t &= e^{\kappa t} x_t\\
	dy_t &= e^{\kappa t} (- \kappa x_t  dt + \sigma dW_t) + \kappa  e^{\kappa t}x_t dt = e^{\kappa t} \sigma dW_t
\end{align}
This yields 
\begin{align}
	x_t = e^{-\kappa t} x_0 + \sigma \sqrt{\frac{1 - e^{-2 \kappa t}}{2 \kappa}} z
\end{align}
where $z \sim N(0,\mathbb I)$. Inspecting the limiting cases of our solution, $x_{t=0} = x_0$ and $x_{t \to +\infty} = \frac{\sigma}{2\sqrt{2\kappa}} z$. So this is a process is a linear interpolation between our input $x_0$ and white noise $z$. 
\begin{sidework}
	Consider the $\kappa=1$ Orstein Ulhbreck SDE.
	\\
	\\
	Moving to the distribution picture, we can see
	\begin{align}
		p_t = (e^{\kappa t} p(\cdot e^{\kappa t})) * g_\sigma
	\end{align}
	where $g_\sigma = \mathcal N(0, 1-e^{-2 \kappa t})$. We can also see this forms a semi-group, we don't get an inverse for free...
	\\
	\\
	Another way to see this is to Fourier transform. Generically if you have $h = f * g$, then $\hat h = \hat f \hat g$. In our case $\hat g(\omega) \sim e^{- (1-e^{-2 \kappa t}) ||\omega||^2/2}$. You can see this expression explodes as time goes on
\end{sidework}
\subsubsection{Denoising}
Now our task is to denoise the OU process. For notation, let's use $y_t = x_{T-t}$ and $q_t = p_{T-t}$.  We can plug our $q_t$ into the Fokker Planck (\ref{eqn:FokkerPlanck}), and derive a time evolution on the distribution for the reverse process
\begin{align}
	\frac{\partial p_t}{\partial t} + \nabla \cdot (- p_t x_t) - \Delta p_t & = 0 & \text{Forward in time}\\
	\implies  \frac{\partial q_t}{\partial t} + \nabla \cdot (q _t y_t) + \Delta q_t & = 0 & \text{Backwards in time}
\end{align}
Notice your have inverse diffusion, which is completely unstable. Show how can we rewrite this to become more stable? Notice the trick
\begin{align}
	\Delta q_t = \nabla \cdot (q_t \underbrace{\nabla \log q_t}_{\text{score}})
\end{align}
We finally get
\begin{align}
	\frac{\partial q_t}{\partial t} + \nabla \cdot (q_t y_t - (1+\alpha) q_t \nabla\log p_t) - \alpha \Delta q_t = 0\\
	dy_t = [y_t + (1+\alpha) \nabla \log q_t] + \sqrt{2\alpha}~ dW_t
\end{align}
We now have a generic way to talk about the reverse-time process!

\subsubsection{Denoising OU}
We have the $\kappa=1$ OU, 
\begin{align}
	x_t = e^{-t} x + \sqrt{1 - e^{-2 t}} z
\end{align}
Taking the coordinate change $\tilde x_t = e^{t x_t}$, you now find at each point in time, you observe $y_t$
\begin{align}
	y_t = x + \sqrt{e^{2t} - 1}z
\end{align}
Recall we are attempting to denoise this process. In signal processing, your trying to minimize the mean square error 
\begin{align}
	\mathbb E_{x,z} [||\hat x(y_t) - x||^2]
\end{align}
So given an observed noisy input $y_t$, can we construct a $\hat x$ (which takes in $y_t$) and yields $x$. We can use \textbf{Tweety's Formula} to construct the optimal denoiser
\begin{theorem}
	Finding the minimum of 
	\begin{align}
		\mathbb E_{x,z} [||\hat x(y_t) - x||^2]
	\end{align}
	Ends up being 
	\begin{align}
		\hat x (x_\sigma) = \mathbb E[x | x_\sigma] =  x_\sigma + \sigma^2 \nabla_{x_\sigma} \log p_\sigma (x_\sigma)
	\end{align}
	Proof:
	\begin{align}
		p_\sigma (x_\sigma) & = \int p(x_\sigma | x) p(x) dx\\
		\nabla p_\sigma (x_\sigma) & = \int g_\sigma (x_\sigma - x) p(x) dx & \text{Due to solution of OU}\\
		& = \frac{1}{\sigma^2} \int  (x_\sigma - x) g_\sigma(x_\sigma - x) p(x) dx\\
		\frac{\sigma^2 \nabla p_\sigma(x_\sigma)}{p_\sigma(x_\sigma)} &  = \int (-x_\sigma + x) p_\sigma(x | x_\sigma) dx & p(x) / p_\sigma(x_\sigma) = p_\sigma(x | x_\sigma)\\
		& = - x_\sigma + \hat x(x_\sigma) 
	\end{align}
	where $g_\sigma(x_\sigma -x ) = C_\sigma e^{-||x_\sigma - x||^2 / 2\sigma^2}$ is a Gaussian distribution. You can now see that the score naturally arises in denoising problems.
\end{theorem}
As a machine learning person, once you see, "minimize square error", you should just plug that into a neural network and let it rip. So you should try to $\min_\theta \ell(\theta) = \min_\theta \mathbb E_{x,z}[|| \hat x_\theta (x_\sigma) - x||^2]$, and then by Tweedy's formula, at inference time you perform $\frac{\hat x_\theta(x) - x}{\sigma^2} = \nabla_{x_\sigma} \log p_\sigma (x_\sigma)$

\subsubsection{Example of Denoising Gaussian}
Consider the distribution $p_\sigma(x) = \frac{e^{- U_\sigma (x)}}{Z_\sigma}$, which means the score is $\nabla_x \log p_\sigma(x) = - \nabla_x U_\sigma(x)$. $p_\sigma$ is Gaussian when $U_\sigma(x) = \frac{1}{2}x^T C^{-1} x$. In this case let's choose a factorization $C \sigma = C + \sigma^2 \mathbb I$. Using Tweedy's Formula, our optimal denoiser (that is $\hat x(x_\sigma) \approx x$) is given by
\begin{align}
	\hat x(\sigma) & = [\mathbb I - (C + \sigma^2 \mathbb I)^{-1} \sigma] x_\sigma \\
	& = (C + \sigma^2 \mathbb I)^{-1} (C + \sigma^2 \mathbb I  - \sigma^2 \mathbb I) \hat x)_\sigma  \\
	& = (C + \sigma^2 \mathbb I)^{-1} C x_\sigma
\end{align}
So in this case, the optimal denoiser is found via a linear regression!

\subsection{Generalization vs Memorization?} 
We want to ask the question does the neural network actually find a score which generalizes beyond the training data? He explains a couple interesting tests \url{https://arxiv.org/abs/2310.02557}

\subsection{Architecture of CNNs}
In a lot of these problems, CNNs (specifically UNets) end up being the more successful architectures. So perhaps we should spend some time to deconstrucitng them.
\\
\\
\subsubsection{Convolutional Operator}
A convolutional operator (in ML), takes an image $(\text{Length, Width, Channels}) \to (\text{Length', Width', Channels'})$. Over pixel space $(\text{Length, Width})$ it applies a convolution, over channel space we apply a linear transformation. The $\ell$'th nodes are given by
\begin{align}
	x_{\ell + 1}(i, c') = \rho \left(\sum_{c \in \text{Channels}} \sum_{\tau \in {\text{\{1,2\}}}} W(\tau, c, c') x_\ell (i - \tau, c)] \right) + b
\end{align}
where $\rho$ is your activation function, $b$ is your bias. The $\sum_{c \in \text{Channels}}$ is the linear transformation between channel, and the $\sum_{\tau \in {\text{\{1,2\}}}}$ is the convolution. Notice that you can effectively reframe the operation inside of $\rho$ as a single linear operator.
\\
\\
Traditionally, as you go deeper into the neural network, people will reduce the spatial dimension and increase the number of channels.

\subsubsection{Explaining Bias in the Model}
Now we want to explore they this architecture produces bias.

If assume there's no bias $b=0$. The tweedy's formula 
\begin{align}
	\hat x = x_\sigma + \sigma^2 \underbrace{\nabla \log \tilde p_\sigma(x_\sigma)}_{\equiv s_\sigma (x_\sigma)}
\end{align}
Since $s_\sigma (x_\sigma)$ is piecewise linear, we can rewrite it as $s_\sigma (x_\sigma) = - \mathcal J_{s_\sigma}(x_\sigma) x_\sigma$. This means we can rewrite Tweedy's formula as
\begin{align}
	\hat x = \left ( \mathbb I + \sigma^2 \mathcal J_{s_\sigma}(x_\sigma) \right)x_\sigma
\end{align}
we can decompose the interior in an eigenbasis.
\begin{align}
	\hat x = \sum_k h_k \langle x_\sigma, \psi_k\rangle \psi_k
\end{align}
This means the difference between prediction $\hat x$ and the true value $x$, we get
\begin{align}
	||\hat x - x||^2 = \sum_{k} | \langle \hat x , \psi_k \rangle - \langle x, \psi_k \rangle | ^2
\end{align}
\begin{sidework}
	Let's work through an example, let's take the probability distribution to be $p(x) = e^{- \frac{1}{2}x^T C_\sigma^{-1}x} / Z$ where $C_\sigma = C + \sigma^2 \mathbb I$. This means $\nabla^2 \log p(x) = -C_\sigma^{-1}$, which has eigenvalues $-\frac{1}{\beta k + \sigma^2}$
\end{sidework}
Mallat then presents some numerical experiments on visualizing these learned basis vectors (see \url{https://arxiv.org/pdf/2310.02557}, same link as the previous one).

\section{Wavlets}
\subsection{Optimal Denoising}
Consider observing a noisy signal $y = x + z$, where $x$ is the true signal, and $z \sim \mathcal N(0, \sigma^2 \mathbb I)$. Our objective is to find a prediction for the true signal $\hat x = H y$ which is a linear function of the noisy signal. We want this predictor to minimize $\epsilon^2 = \mathbb E[|| H y - x ||^2]$.
\begin{theorem}
	[Wiener Filter] The Winer Filter is
	\begin{align}
		H = (C + \sigma^2 \mathbb I)^{-1}C
	\end{align}
	It has eigenvalues \begin{align}
		\frac{\beta k}{\beta k + \sigma^2} \in [0,1]
	\end{align}
	
	You can show that this predictor minimizes the correlation $\mathbb E[(\hat x - x) y]$
\end{theorem}
\begin{definition}
	[PCA Basis] Consider the basis vectors $\{\psi_k\}_k$ comparised of wavelets
	\begin{align}
	\psi_k(n) & = \frac{1}{\sqrt{d}} e^{i 2\pi n k/d}\\
	\langle C \psi_{k'}, \psi_k\rangle & = \beta k = \mathbb E[|\langle x, \psi_k \rangle|^2]
	\end{align}
	
\end{definition}
So, now we can change of basis our true signal $H y = \sum_k h_k \langle y, \psi_k\rangle \psi_k$. So now how do we construct the coefficients $h_k$?

\begin{align}
	x & = \sum_k \langle x, \psi_k \rangle \psi_k\\
	\mathbb E ||Hy - x ||^2 & =  \sum_k \mathbb E \Big| \Big | (\lambda_k(x)  + 1) \langle x, \psi_k \rangle + h_k \langle z, \psi_k \rangle \Big| \Big|^2 & \text{Using } y = x + z\\
	& =  \sum_k  \Big| \Big | (\lambda_k(x)  + 1) \langle x, \psi_k \rangle + h_k \sigma^2 \Big| \Big|^2
\end{align}
Now I can optimize to find the minimum of this quantity...
\begin{align}
	0 = \partial_{h_k} \mathbb E ||Hy - x ||^2 &  \implies h_k = \frac{|\langle x, \psi_k \rangle|^2}{|\langle x | \psi_k\rangle|^2 + \sigma^2} & \text{Minimizer}\\
	\therefore \epsilon & = \sum_k \frac{ \sigma^2 |\langle x, \psi_k \rangle|^2}{|\langle x, \psi_k \rangle|^2 + \sigma^2}
\end{align}

\subsubsection{Binary Basis}
Ok how does the error change if I only allow for binary decisions on the coefficients (that is you either have that basis element on or off)
\begin{align}
	\lambda_k = \begin{cases}
		1 & |\langle x, \psi_k \rangle| \geq \sigma\\
		0 & |\langle x, \psi_k \rangle| < \sigma
	\end{cases}
\end{align}
What ends up happening is
\begin{align}
	\epsilon^2 = \sum_k [(\lambda_k - 1)^2 |\langle x, \psi_k \rangle|^2 + \lambda_k \sigma^2]
\end{align}
If we use the fact $\frac{1}{2}\min(a,b) \leq \frac{ab}{a+b} \leq \min(a,b)$, we can show 
\begin{align}
	\epsilon_m \leq \epsilon^2 \leq 2 \epsilon_m
\end{align}
so basically you only loose a factor of two...

\subsubsection{Thresholded}
What if you consider the expansion
\begin{align}
	Hy = \sum_k \text{sth}_T(\langle y, \psi_k \rangle ) \psi_k\\
	\text{sth}_T(a) = \begin{cases}
		a - \text{sign}(a) T & |a| > T\\
		0 & |a|< T
	\end{cases}
\end{align}
If we set $T = \sigma \sqrt{2 \log d}$, you can show
\begin{align}
	\epsilon_m \leq \epsilon \leq (2 \log d +1) [\epsilon_m + \sigma^2]
\end{align} 
You can also show that you cannot do asymptotically better as $d \to \infty$. So this choice is optimal.

\subsection{Compression}
In practice, denoising and compression end up being similar tasks. So if you have a good denoising basis, you should try it out on compression. Why is this? Consider the problem where e want to minimize the error
\begin{align}
	\epsilon _m \sim \epsilon_b & = \sum_k \min(|\langle x, \psi_k \rangle|^2 , \sigma^2)\\
	& = M \sigma + 2 \sum_{|\langle x, \psi_k | > \sigma^2} |\langle x, \psi_k \rangle |^2
\end{align}
So imagine you want to construct a approximation $x_M$ for signal $x$ 
\begin{align}
	x = \sum_{k=1}^d \langle x, \psi_k \rangle \psi_k\\
	x_M = \sum_{k \in I_M} \langle x, \psi_k \rangle \psi_k\\
	\therefore || x - x_M ||^2 = \sum_{k \notin I_M} |\langle x, \psi_k \rangle|^2
\end{align}
Well $I_M = \{ k: |\langle x, \psi_k \rangle | \geq \sigma \}$

\section{Wavelet Slides}
Since we're trying to denoise a time series signal. There are often discontinuous jumps, yet high frequency noise. So we want to construct a basis where we can pick out high frequency noise, yet also have short time scale. Fourier transforms work entirely in frequency or time basis, wavelets allows us to go "somewhere in between"...\\
\\
Let's begin with the simple Haar wavelet
\begin{align}
	\{ \psi_{j,n}(t) = \frac{1}{\sqrt{2^j}} \psi( \frac{t - 2^j n}{2^j})\}_{(j,n) \in \mathbb Z^2} 
\end{align}
This forms an orthogonal basis of $L^2(\mathbb R)$

\section{Renormalization Group \& Hierarchies}
We start by recalling physics. Consider a physical state at equilibrium $x \in \mathbb R^d$. You'll measure the states according to a Gibbs distribution
\begin{align}
	p_t(x) = \frac{e^{-U_t(x)}}{Z}
\end{align}
Our goal is to find a parameterized version of $U(x)$. Often $U(x)$ is not convex, the associated hessian is singular (non-invertible), and it'll emit long-range correlations. Thinking about these correlations, this will depend on the "scale" that we probe at, which we'll denote by $t$. This motivates the usage of the \textbf{renormalization group}.
\\
\\
There are perhaps naive ways of performing RG (Kadnoff RG / Real Space RG).
\\
\\
Suppose your ansatz $U_t(x) = \langle O_t, \phi(x)\rangle$. So now let's ask the question, how do we go from $p_{j-1} \to p_j$ (that is change scales). Let $x_{j-1} = (x_j, \bar x_j)$ (kept variables, and complement of kept variables which are orthogonal) (orthognality is prevent Jacobian from popping out). We can now simply integrate out $\bar x_j$ and have the effective theory
\begin{align}
	\int p_{j-1}(x_j, \bar x_j) d\bar x_j = p_j(x_j)
\end{align}
and we'd hope that calculation of sufficient statistics is preserved
\begin{align}
	\langle \mathcal O(x_j)\rangle_{p_j} = \langle \mathcal O(x_j, \bar x_j) \rangle_{p_{j-1}} & & \text{(which holds up until the "scale")}
\end{align}

\begin{sidework}
	In Wilsonian RG, you end up performing the calculation
\begin{align}
	p_{j-1}(x_{j-1}) = p_{j-1}(x_j, \bar x_j) = p_j (x_j) \bar p_j (\bar x_j | x_j)
\end{align}
where you have created $\bar{p}_j$ s.t. it is convex and local. \\
\\
For notational simplicity let's denote
\begin{align}
	\bar p_j(\bar x_j | x_j) = \frac{e^{-\bar U_j(\bar x_j , x_j)}}{\bar Z_j}
\end{align}
and now you can see $U_{j-1}(x_{j-1}) = U_j(x_j) + \bar U_j(\bar x_j, x_j)$.\\
\end{sidework}
In diffusion, you never want to calculate the transition between $p_{t=0}$ and $p_{t=T}$ (Gaussian to target distribution), we want to break it up into little steps (aka annealing)
\begin{align}
	p_0(x_0) = p_T(x_T) \prod_{j=T}^1 \bar p_j(\bar x_j | x_j)
\end{align}
So intead of computing $O_0$ in $U_0 = \langle O_0, \phi(x)\rangle$. You want to instead of compute $\{O_s : \bar O_j\}_{1 \leq j \leq T}$.


\subsubsection{Sampling}
We use Langevin 
\begin{align}
	dx_t = \nabla \log p(x) + \sqrt{2} dW_t
\end{align}
When you're using a maximum entropy model $p(x) \propto e^{-U(x)}$
\begin{align}
	dx_t = - U(x) + \sqrt{2}dW_t
\end{align}
\begin{itemize}
	\item If $U(x)$ is convex, your mixing time is proportional to the condition number of the Hessian (associated with $U$)
	\item If $U(x)$ is not convex, it'll still converge, but it'll take a long time...
\end{itemize}
\begin{sidework}
	For example, we can consider using wavlets on a gaussian
	\begin{align}
		p_0(x) = \frac{e^{-U}}{Z}
	\end{align}
	Where $U(x) = x^T K x /2 = (K)^T (\frac{1}{2}x^T x) = \langle O, \phi(x)\rangle$. This means $U(x)$ is convex, and therefore mixing time is set by the condition number is set by $K$. \\
	\\
	\textcolor{blue}{[Mallat makes a point that wavelets keep this condition number invariant as you vary the scale... Meaning, perform the orthognal decomposition $x_{j-1} = (x_j, \bar x_j)$ using wavelets, and now your $K_{j-1} \to \bar K_j$ should keep the same condition number. This seems a bit nontrivial to me, and I'm not sure how to see this...]}\\
	\\
	In physics this is called renormalization. In machine learning language this is called batch-normalization.
\end{sidework}

\subsubsection{Training}
We perform maximum likelihood to find the parameters. We accomplish this by minimizing the KL divergence. 
So let the loss function be
\begin{align}
	\ell(\theta) & = \text{KL}(p |p_\theta)\\
	& = - \mathbb E_{x \sim p}[\log p_\theta] + (\text{Constant w.r.t. }\theta)
\end{align}
If $U = \langle \theta, \phi(x)\rangle$, then $\nabla^2 \ell(\theta) = \text{Cov}_{p_\theta}[\phi]$

\section{Continuous Ising, $\phi^4$ Model}
Consider the target distribution defined by $U_0(x) = \frac{1}{2} x^T K x + \sum_i V(x(i))$. In particular let's focus on the case $V(x) = \sum_a \alpha_k p_k(a)$. So in the $U = \langle O, \phi(x)\rangle$ formulation, $O = (K, \alpha_k)$ and $\phi(x) = (x^T x, \sum_i x^k(i))$. \\
\\
Experiments in \url{https://arxiv.org/abs/2207.04941} show that this works well!

\section{Can we use this in other things?}
See Wavelet Score-Based Generative Modeling, Learning Multi-scale local conditional probability model of images, scattering spectral models for Physics.









































\newpage
hi



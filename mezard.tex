\section{Overview}
\begin{enumerate}
	\item Langevin-Fokker Planck \& Ornstien Uhlenbeck
	\item Principles of Generative Diffusion
	\item A simple case: Gaussian Data
	\item Aside: Related approaches ODE and Stochastic Localization \& Interpolants
	\item Intermission: Thermodynamic Score
	\item A harder case: Curie Weiss
	\item Speciation Transition: Classifier Free Guidance
	\item Generalization vs Memorization
\end{enumerate}

\section{Recall: Stochastic Processes}
\subsection{Langevin Equation}
Let $x \in \mathbb R^d$, and has the state equation
\begin{align}
	\frac{dx}{dt} = F(x) + \eta(t)
\end{align}
where $F: \mathbb R^d \to \mathbb R^d$ is the force field, and $\eta(t)$ is a Gaussian with properties $\langle \eta(t)\rangle = 0$ and $\langle \eta(t) \eta(t') \rangle = 2 \delta(t - t')$. In stochastic process notation it is
\begin{align}
	dx_t = f(x) dt + \sqrt{2	} dW_t
\end{align}
\begin{sidework}
	In general, Mezard will work in continuous time. However whenever we want to make a comment about implementations on a computer, we'll use the Ito descretization.
	\begin{align}
		x(t + \delta t) = x(t) + \delta t ~F(x(t)) + \int_{t}^{t + \delta t} d\tau \eta~ (\tau)
	\end{align}
	and
	\begin{align}
		\delta x = \delta t F(x(t)) + \sqrt{2 \delta t} z_t
	\end{align}
	where $z_t$ is a unit gaussian.
\end{sidework}
\begin{sidework}
	Note the integral of the time-dependent gaussian is given by
	\begin{align}
		\langle y \rangle := \int_t^{t+\delta t} \eta (\tau) d \tau
	\end{align}
	has the following properties $\langle y \rangle = 0$ and $\langle y^2\rangle = 2 \delta t$.
\end{sidework}
\subsection{Fokker Planck}
Consider the conditional distribution $p_t(x | x_0)$, the equation which keeps it probabliity distribution is the Fokker Planck equation
\begin{align}
	\frac{\partial p_t}{\partial t} = \Delta p_t - \nabla \cdot  (F p_t)
\end{align} 
where $F$ is a vector field. You can then ask what's condition of the stationary distribution, that is $\partial_t p_t = 0$, it must satisify
\begin{align}
	\Delta p_t + \nabla \cdot (p \nabla V) = 0\\
	p_{stationary}(x') = \frac{1}{Z} e^{-V(x)}
\end{align}
Note that our usage of $\sqrt{2}$ in the previous section was to ensure we get a temperature $\beta = 1$.

\subsection{Ornstein Uhlenbeck}
This is when $V(x) = \frac{1}{2} |x|^2$. So the Langevin equation becomes
\begin{align}
	\dot x = F(x) + \eta(t) \implies \dot x = - x + \eta(t)
\end{align}
and your stationary distribution becomes
\begin{align}
	p_{st}(x) = e^{-|x|^2 / 2}
\end{align}
\begin{sidework}
	For example, if you have $x(t=0) = a \in \mathbb R^d$. Then the solution to the Langevin becomes
	\begin{align}
		x(t) = a e^{-t} + \int_0^t e^{-(t-\tau)}\eta(\tau) d\tau
	\end{align}
	the second term is equivalent to the random variable $\mathcal N(0, 1-e^{-2t})$, and we'll notate $\Delta t = 1-e^{-2t}$.
\end{sidework}
So you can interpret this process as something which always goes to white noise.
\subsection{General Time and Variance}
Consider the equation
\begin{align}
	\frac{dx}{dt} = f(t) x(t) + g(t) \eta(t)
\end{align}
previously $g = 1$, but now $f: \mathbb R^d \to \mathbb R^d$ and $g: \mathbb R^d \to \mathbb R^d$. Solving the equation with the same boundary condtion as before ($x(t=0) = a)$, you get
\begin{align}
	x(t) & = a s(t) + s(t) \sigma(t) z(t)\\
	& \text{s.t. }\begin{cases}
		s(t) = \exp\left[ \int_0^t d\tau ~f(\tau) \right]\\
		\sigma(t)^2 = 2 \int_0^t d\tau \left (\frac{g(\tau)}{s(\tau)} \right)^2
	\end{cases}
\end{align}
\begin{sidework}
	Example: Brownian Motion is recovered when $f=0$ and $g = 1$.
\end{sidework}

\section{Principals of Generative Diffusion}
\begin{sidework}
	For some background reading for this section there's: Sohl-Dickstein et al (2015), Yang Song \& Stefano Ermon (2019), and a Review by Ling Yang et al (ArXiv:2209.00796). 
\end{sidework}
In generative modeling, \emph{the problem setup} is that you have an a target distribution $p_0(a)$ s.t. $a\in \mathbb R^d$, however you only have an empirical estimation of said target distribution $\{a^\mu\}_{\mu=1}^n$ (and was assume $a^\mu$ was sampled iid from the target distribution). Your goal is to use a ML to reconstruct \& sample from the target distribution using only knowledge from the empirical distribution.

\subsection{Forward Process}
Let $a \in \mathbb R^d$ (where $a \sim p_0$), and $x(t=0) = a$. The forward process is the OU process, which we recall as 
\begin{align}
	\dot x = -x + \eta(t) \implies x(t) = a e^{-t} + \sqrt{\Delta_t} z_t
\end{align}
where $\Delta_t = 1 - e^{-2t}$.
At time $t$ we say that $x \sim p_t(x)$ which has time evolution of the Langevin
\begin{align}
	\frac{\partial p_t}{\partial t} = \Delta p_t + \nabla \cdot (x p_t) \implies p_t(x) = \frac{e^{-(x-ae^{-t})/2\Delta_t}}{(2\pi \Delta_t)^{d/2}} =  \mathcal N(x; a e^{-t}, \Delta_t)
\end{align}
This means that if $a \sim p_0$, then we have a joint probabity distribution over $a$ and it's (partially) noised counter part $x$.
\begin{align}
	p_t(a,x) &= p_0(a) \mathcal N(x_t; ae^{-t} , \Delta_t)\\
	p_t(x) &= \int da ~ p(a,x)\\
	p(a | x_t) &= \frac{p(a)  \mathcal N(x; ae^{-t} , \Delta_t)}{\int d a' p_0(a') \mathcal N(x_t; a e^{-t}, \Delta t)}
\end{align}

\subsection{Backwards Process}
To notate the backwards process, let's notate our time notation. Previously $t$ starts at 0, and ends at $t_f$. In our backwards process, $\tau$ starts at 0 (which corresponds to $t=t_f$) and ends at $\tau_f$ (which correspond to $t=0$). So $\tau = t_f -t$. If we plug in this coordinate change into our Langevin
\begin{align}
	-\frac{\partial p}{\partial \tau} & = \Delta p + \nabla \cdot(x p) \implies \frac{\partial p}{\partial \tau}  = -\Delta p - \nabla (x p) \label{eqn:Reverse_Langevin}
\end{align}
Ok, restarting from the general Fokker Planck
\begin{align}
	\frac{\partial p}{\partial \tau} = \Delta p - \nabla (F p)
\end{align}
What force-field $F$ do we need to recover the backwards process (\ref{eqn:Reverse_Langevin}). If you ansatz $F = x + 2 \nabla \log p$, you can recover the right thing. In general, if you want to undo the forward process (of OU process), the probability in the score is taken to be $p_{t_f-\tau }^F(x)$ (the probability of the forward process).
\begin{sidework}
	A comment on notation. The forward probability distribution $P_t^F(x)$ and the backwards probability distribution $P_\tau^B(x)$ are related via
	\begin{align}
		P_t^F(x) = p_{t_f - t}^B(x)
	\end{align}
\end{sidework}
In the case $t_f \gg 1$
\begin{align}
	p_{t_f}^F (x) & = \frac{e^{-x^2/2}}{Z}\\
	\log p & = - \frac{x^2}{2} + C\\
	\nabla \log p & = -x
\end{align}
In the case $t \simeq t_f \gg 1$
\begin{align}
	F(x) = x + 2 \nabla \log p = -x
\end{align}
\subsection{Comment on Discretization}
See Ho Jain Abbeel (NeurIPS 2020).


\subsection{The Score}
\begin{theorem}
	[Tweedie's Formula]
\end{theorem}
Consider your probability $p_t(x) = \int da ~p_0(a) \frac{ e^{-(x-a e^{-t})^2 / (2 \Delta_t)}}{\sqrt{2 \pi\Delta_t}}$. This means the joint probability $p_t(a,x) = p_0(a) \mathcal N(x; a e^{-t}, \Delta_t)$ (where $a$ is $x_{t=0}$). The score of $p_t(x)$ becomes
\begin{align}
	\varphi = \nabla_x \log p_t(x)&  = \frac{1}{p_t(x)} \int da~ p_0(a) \frac{-(x-ae^{-t})}{\Delta_t} \frac{e^{-(x-ae^{-t})^2}/(2\Delta_t)}{\sqrt{2\pi \Delta_t}} \\
	& = - \frac{x}{\Delta_t} + \frac{e^{-t}}{\Delta_t} \langle a \rangle_{x,t}
\end{align}
where $\langle a \rangle_{x,t} = \int da ~ a~ P(a | x,t) = \frac{\int da~ a~p_0(a) e^{-(x-a e^{-t})^2/2\Delta t}}{\int da ~p_0(a) e^{-(x-a e^{-t})^2/2\Delta t}}$
\subsection{Approximating the Score}
Say $p_0$ is unknown, but we have a dataset $\{a^\mu\}_{\mu=1}^n \sim_{iid} p_0$. We will parameterize an estimate of the score $\varphi_\theta(x,t)$, so now we need a loss function to minimize. A typical loss function is
\begin{align}
	\mathcal L = \int_0^{t_f} \lambda(t) \underbrace{\mathbb E_{x \sim p_t}[|\varphi_\theta(x,t) - \nabla \log p_t(x)|^2]}_{L^t}
\end{align} 
The $\lambda(t)$ is a Lagrange multiplier throughout time. Let's inspect the $L^t$ term. If you integrate by parts you can show
\begin{align}
	L^t & = \mathbb E_x [|\varphi_\theta(x,t)|^2 - 2\varphi_\theta(x,t)\cdot \nabla \log p] + \text{Constant}\\
	-2 \mathbb E[\varphi_\theta(x,t) \cdot \nabla \log p] & = -2 \int dx \int da p_0(a) \frac{x - ae^t}{\Delta_t} \frac{e^{-(x-a e^{-t})^2 / 2\Delta_t}}{\sqrt{2\pi \Delta_t}} \varphi_\theta(x,t)\\
	\therefore L^t &= \mathbb E_{a\sim p_0} \mathbb E_{x\sim p_{OU}(x|a)} \Big [\Big|\varphi_\theta(x,t) + \frac{x - a e^{-t}}{\Delta_t}\Big|^2 \Big]
\end{align} 
Because the OU process has the dynamics
\begin{align}
	x = a e^{-t} + z \sqrt{\Delta_t} \implies \frac{x -ae^{-t}}{\Delta_t} = \frac{z}{\sqrt{\Delta_t}}
\end{align}
where $z \sim \mathcal N(0,1)$, this implies that $L^t$ is heuristically $\mathbb E_z [|\varphi_\theta(a e^{-t} + z\sqrt{\Delta_t}, t) + z|^2]$, so you can interpret this as matching noise $z$!
\subsection{Various Scores}
\subsubsection{Perfect Score}
Consider the model
\begin{align}
	p_0(a) = \frac{e^{-E(a)}}{Z}, ~ \varphi(x,t) = \frac{1}{\Delta_t} (-x + e^{-t}\langle a \rangle_{x,t})
\end{align}
asdf
\begin{align}
	p(a|x,t) = \frac{1}{Z(x)} p_0(a) e^{-(x - a e^{-t} )^2/ 2 \Delta_t}
\end{align}

\begin{sidework}
	See KPZ and Burger's Equation...
\end{sidework}

\subsubsection{Empirical Score}
If you only have a database $\{a^\mu\}_{\mu=1}^n$, the best thing you can do is $p_0^{emp} = \frac{1}{n} \sum_{\mu=1}^n \delta(a - a^\mu)$. This means our OU processed empirical becomes
\begin{align}
	p_t^{emp}(x) = \frac{1}{n} \sum_{\mu=1}^n \frac{e^{(x-a^\mu e^{-t})^2 / 2\Delta_t}}{(2\pi \Delta)^{n/2}}
\end{align}
Notice this is just a Gaussian mixture model. The empirical score is
\begin{align}
	\varphi^{emp}(t,x) & = \nabla \log p_t^{emp}(x) \\
	& = \sum_\mu - \frac{x - a^\mu e^{-t}}{\Delta_t} e^{-(x - a^\mu e^{-t})^2/2\Delta_t} \Big / \sum_\mu e^{- (x - a^\mu e^{-t})^2 / 2\Delta_t} 
\end{align}

\subsubsection{Fitted Score}
\begin{align}
	\mathcal L & = \mathbb E_{a,x \sim p_t(a,x)} [ \varphi_\theta(x,t) + \frac{x - a e^{-t}}{\Delta_t}|^2\\
	& = \frac{1}{n} \sum_\mu \mathbb E_{z \sim \mathcal N(0,1)} \left [ \varphi_\theta(x^\mu, t) + \frac{z}{\sqrt{\Delta_t}} \right]
\end{align} 
where $x^\mu = a^\mu e^{-t} + z \sqrt{\Delta_t}$ is the input noised via OU process.\\
\\
Unfortunately there's not much more we can do with this, we must now put it on the computer and see what comes out.

\subsubsection{More General Time + Variance}
Instead of just using OU process, what if we noise the image according to
\begin{align}
	\frac{dx_i}{dt} = f_i(t) x_i + g_i(t) \eta_i(t)
\end{align}
where $x = (x_1, ..., x_d)$ and boundary condition $x(t=0) = a_i \sim p_0(a)$. This yields a PDE on the probability $\text{Law}(x(t)) \sim p_t$, this is
\begin{align}
	p_t(x) & =  p_0(a) \frac{e^{-(x - a s(t))^2/ 2s(t)^2 \sigma(t)^2}}{\sqrt{2 \pi s(t) \sigma(t)}}\\
	\implies \varphi(x,t) = \nabla \log p_t(x) & =  - \frac{x}{s(t)^2 \sigma(t)^2} + \frac{s(t)}{s(t)^2 \sigma(t)^2} \langle a \rangle_{x,t}
\end{align}


\section{A First Example: Gaussian Data}
Consider your data's distribution is $p_0 = \mathcal N(m, \Sigma)$. We'll notate $\sum_{ij} a_i \Sigma_{ij}a_j = \langle a | \Sigma | a \rangle$ using bra-ket notation.  So this means our noised distribution is
\begin{align}
	p(a,x,t) = \frac{e^{-\frac{1}{2} \langle a-m  | \Sigma^{-1}  | a - m \rangle}}{\sqrt{2\pi}^d \sqrt{\det \Sigma}} \frac{e^{-\frac{1}{2 \Delta_t}(x - ae^{-t})^2 }}{\sqrt{2\pi \Delta_t}^d}
\end{align}
So our marginalized distribution on $p_t(x)$ becomes
\begin{align}
	p_t(x) = \int da~ p(a,x,t)= \mathcal N(m e^{-t}, \Gamma_t)
\end{align}
where $\Gamma_t = \Delta_t \mathbb I_d + e^{-2t} \Sigma$. 
\subsubsection{Perfect Score}
\begin{align}
	\nabla \log p_t = - (\Gamma_t^{-1})(x - m e^{-t})
\end{align}
In the case when $t \gg 1$: you recover $\nabla \log p_t = -x$ (so it's a Gaussian!). In $t \ll 1$: $\nabla \log p_t \sim (\Sigma...)$ 

\subsubsection{Optimized Score}
Now your objective is to construct a parameterized score $\varphi_\theta$ from data $\{a^\mu\}_\mu \sim_{iid} p_0$. If you're smart you know that the answer must be linear, so let's ansatz our parameterized score as
\begin{align}
	\varphi_\theta(x,t) = - W x - b \label{eqn:18.0.2_Ansatz}
\end{align}
Now we can argmin the loss. However, we'll assume the case when $m = 0$ and $b=0$.
\begin{align}
	\mathcal L(t) = \mathbb E_{a \sim p_0^{emp}} \mathbb E_{x \sim \mathcal N(a e^{-t} , \Delta_t \mathbb I)} | \varphi_\theta(x,t) = \frac{x - a e^{-t}}{\Delta_t}|^2
\end{align}
where $x = a^\mu e^{-t} + \sqrt{\Delta_t} z$ is the noised initial data according to OU. Plugging in our ansatz (\ref{eqn:18.0.2_Ansatz}), we get
\begin{align}
	|\varphi_\theta(x,t) + \frac{x- a^\mu e^{-t}}{\Delta_t}|^2 & = |- W (a^\mu e^{-t} + \sqrt{\Delta_t} z) + \frac{z}{\sqrt{\Delta_t}}|^2\\
	L^t = \mathbb E_z [|\varphi_\theta(x,t) + \frac{x- a^\mu e^{-t}}{\Delta_t}|^2] & =  \frac{1}{\Delta_t} \text{Tr}[(\mathbb I - \Delta_t W)^T (\mathbb I - \Delta_t W)]  + \frac{e^{-2t}}{\Delta_t} \langle a^\mu | W^T W | a^\mu\rangle
\end{align}
We note that $\langle a^\mu |W^T W | a^\mu\rangle = \text{Tr} (W^T W C^e)$ where $C^e_{ij} = \frac{1}{n} \sum_\mu a_i^\mu a_j^\mu = \frac{1}{n} aa^T$ is the empirical correlation function. Finally let's optimize over $W$ to find the minimum of the loss
\begin{align}
	\frac{\partial L^t}{\partial W} = 0 \implies W_{opt} = [(1 - e^{-2t}) \mathbb I + e^{-2t} C^e]^{-1}	
\end{align}
So in the limit as your amount of data $n \to \infty$, you find that $C^e \to \Sigma$ and $W_{opt} \to W_{perfect}$.
\begin{sidework}
	We can attempt to generalize some of these results using Random Matrix Theory.\\
	\\
	Say for example the covariance matrix $\Sigma$ gets some Gaussian Noise. Meaning I observe an empirical noisy covariance $H$
	\begin{align}
		H = \Sigma + \frac{1}{d^{a/2}} G
	\end{align}
	where $G \sim \text{GOE}(d)$. There are known results on this (since $\Sigma \approx aa^T$ is a rank-1 perturbation).
	\begin{itemize}
		\item When $a > 1$: Eigenvalues of eigenvectors of $H$ are on the order of eigenvalues/vectors of $\Sigma$. Only in this case will your reconstruct the original distribution.
		\item If $a \in (0,1)$, the eigenvalues of $H$ are on the order of $\Sigma$, however the eigenvectors differ in directions.
	\end{itemize}
	The lesson to be learned is that, what you find for Isotropic Gaussian, is not same for high-covariance Gaussian.
\end{sidework}

\section{Related Approaches i.e. ODE, Stochastic Localization, \& Interpolants}
\subsection{Ordinary Differential Equations}
Consider the stochastic differential equation
\begin{align}
	\dot x = F(x) + \eta(t)
\end{align}
where $\langle \eta \rangle = 0$ and $\langle \eta(t) \eta(t') \rangle = 2 \delta(t- t')$. It has an associated time evolution on $\text{Law}(x_t) = p_t$
\begin{align}
	\frac{\partial p}{\partial t} = \Delta p - \nabla (F p) = \nabla (F p - \nabla p)
\end{align}
So if we introduce the function $g(x) = F(x,t) - \varphi(x,t)$ (where $\varphi = \nabla_x \log p_t(x)$ denotes the score). Then you get the \textbf{continuity equation} (Mezard calls this a ``flow equation'').
\begin{align}
	\frac{\partial p}{\partial t} + \nabla (g(x,t) p) = 0
\end{align}
Using this transformation, we now see our forward stochastic differential equation is
\begin{align}
	\dot x = g(x,t)
\end{align}
\begin{sidework}
	There's an interesting paper (\url{https://arxiv.org/abs/2006.00702}) which shows how to estimate the solution to the Fokker Planck equation by using a mean-field limit of evolving many particles. Could be cool.
\end{sidework}
Anyways, what is the flow for the backwards process
\begin{align}
	\frac{dx}{d\tau} = F^B(x,\tau) = \eta(\tau)\\
	F^B (x,\tau) = 2 \varphi(x,\tau)\\
	\frac{dp^B}{d\tau} + \nabla(p^B (F^B(x,\tau) - \nabla \log p^B)) = 0\\
	\implies \dot x = x + \varphi(x,t)
\end{align}
\begin{sidework}
	So you might be tempted to say we should do this over diffusion, as you don't have to sampling random numbers in the dynamics... However it is (said in the literature to be) harder to sample multimodal distributions.
\end{sidework}
\subsection{Example with Isotropic Gaussian}
Consider the target distribution
\begin{align}
	p_0(a) = \frac{e^{-|a|^2/2 \sigma^2}}{\sqrt{2\pi \sigma^2}}
\end{align}
The interpolated distribution (via OU process) is
\begin{align}
	p_t(x) = \frac{e^{|x|^2/2\Gamma_t}}{\sqrt{2\pi \Gamma_t}} \implies \varphi(x,t) = \nabla_x \log p_t(x) = -x/\Gamma_t
\end{align}
And our ODE is
\begin{align}
	\dot x = \frac{x}{1 + (\sigma^2 - 1) e^{-2 (t_g - \tau)}}
\end{align}

\subsection{Stochastic Interpolants}
For the original method, see Albergo \& Vanden-Eijnden.  What they realized is that we start with some inital distribution $p_0$ and we map it to a final gaussian $p_{final} \propto e^{-x^2/2}$. \\
\\
At each time $t$ , you generate $x(t) = \alpha(t) a + \beta(t) z$ (where $z \sim \mathcal N(0,\mathbb I)$). And since you want to recover $x_{t=0} \sim p_0$ and $x_{t=t_f} \sim p_{final}$ we assert boundary conditions $\alpha(0) = 1, \beta(0) = 0$ and $\alpha(t_f) = 0 , \beta(t_f) = 1$, which asserts $x(0) = a$ and $x(t_f) = z$. We denote $\alpha,\beta$ as the \textbf{interpolant}. \\
\\
So what you should have noticed is that we don't assert any Langevin. We just assert our interpolation scheme. This allows for flexibility in the engineering process! If we assert $\text{Law}(x_t) = p_t(x)$, we can once again derive a time evolution on the probability distribution
\begin{align}
	p_t(x|a) & = \frac{e^{-\frac{1}{2\beta(t)^2}|x-\alpha(t) a|^2}}{\sqrt{2\pi \beta(t)^2}^d}\\
	p(a|x,t) & = \frac{1}{Z}p_0(a) e^{-\frac{1}{2\beta(t)^2} |x-\alpha(t) a|^2}
\end{align} 
And this probability distribution satisfies
\begin{align}
	\frac{\partial p_t}{\partial t} + \nabla (b(x,t) p_t(x)) = 0 \label{eqn:stochasticInterpolant_probability}
\end{align}
where $b(x,t) = \mathbb E_{a,z} [\dot \alpha a + \dot \beta z | \alpha a+ \beta z = x]$. 
\begin{sidework}
	Let's prove this!
	\begin{align}
		p_t(x) = \int da~ p_0(a) \int \mathcal Dz \delta(x - (\alpha(t) a + \beta(t) z))
	\end{align}
	Plugging this into the continuity equation
	\begin{align}
		\frac{\partial p_t(x)}{\partial t} & = - \int da~ p_0(a) \int \mathcal Dz ~[\dot \alpha(t) a + \dot \beta(t)z] \dot \nabla \delta(x - (\alpha(t) a + \beta(t) z))\\
		& = - \nabla \cdot \left[ p(x,t) \int da ~p_0(a) \int \mathcal Dz [\dot \alpha a + \dot \beta z] \frac{\delta (x - (\alpha a + \beta z))}{p(x,t)} \right]
	\end{align}
	So you'll notice, $\int da ~ p_0(a) \int \mathcal Dz~[\dot \alpha a + \dot \beta z] \frac{\delta(x - (\alpha a  +\beta z)}{p(x,t)} = \mathbb E_{a,z} [ \dot \alpha a  + \dot \beta z | \alpha a  + \beta z = x]$, which is $b(x,t)$.\\
	\\
	QED.
\end{sidework}
Why bring this up? Well diffusion is just a subset of potential stochastic iunterpolants where
\begin{align}
	z = \frac{x - \alpha a}{\beta}, b(x,t) = \frac{e^{-2t}}{\Delta _t x - \frac{e^{-t}}{\Delta_t}} \langle a \rangle_x
\end{align}

\subsection{Unifying Framework for Langevin, ODE, and Stochastic Interpolants}
Starting from the continuity equation for stochastic interpolants (\ref{eqn:stochasticInterpolant_probability}). If we ansatz $b$
\begin{align}
	b_\lambda^F(x,t) = b(x,t) + \lambda \varphi(x,t)
\end{align}
Then the continuity equation becomes
\begin{align}
	\frac{\partial p_t}{\partial t}  + \nabla\cdot (b_\lambda^F(x,t) p_t) = \lambda \nabla \cdot (\varphi(x,t) p_t) = \lambda \Delta p_t
\end{align}
You now have a Fokker Planck equation which emits a SDE interpretation
\begin{align}
	\frac{dx}{dt} = b_\lambda^F(x,t) + \sqrt{\lambda} \eta(t)
\end{align}
If we look at a couple cases
\begin{itemize}
	\item $\lambda = 1$ we recover our usual Langevin
	\item $\lambda = 0$ we recover ODE (flow based methods)
\end{itemize}

\subsection{Stochastic Localization}
For the original, see R. Eldan (2013, 2020, 2022); El Aloni Montanar Selke (2022); Montanari (2023).\\
\\
Montanari's "interpolant" is of the form
\begin{align}
	x(\tau) = a\tau + \sqrt{\tau} z
\end{align}
where $z \sim \mathcal N(0, \mathbb I)$. You now find that
\begin{align}
	p_\tau(x|a) = \frac{1}{\sqrt{2\pi \tau}^d} e^{-\frac{\tau}{2}(\frac{x}{\tau} - a)^2}
\end{align}
When I look at the limit $\tau \to \infty$, you get $\frac{x}{\tau} \to a$. You can think of this as performing a random walk where you converge to $a$, but your variance explodes.\\
\\
Looking at the conservation equation
\begin{align}
	\frac{\partial p_\tau(x)}{\partial \tau} + \nabla \cdot (bp) = 0\\
	\text{s.t. } b(x,\tau)= \mathbb E[a + \frac{1}{2\sqrt \tau} z | a \tau + \sqrt{\tau} z + x]  = \frac{x}{2\tau} + \frac{1}{2}\langle a \rangle_{x,\tau}
\end{align}
And once again using Fokker Planck, you get the SDE
\begin{align}
	\dot x = \langle a \rangle_{x,t} + \eta(\tau)
\end{align}
\subsubsection{Example on EBM}
Consider the exponential family distribution $p(a) = e^{- \beta E_0(a)}/Z$ where $\sum_{i=1}^d a_i^2 = d$ and $E_0(a) = - \sum_{i < j < k} J_{ijk} a_i a_j a_k$ (which is a $p=3$ spin glass).\\
\\
We know as a function of temperature, we know there's a low temperature spin glass phase ($\beta > \beta_{k}$),  high temperature phase $(\beta < \beta_{d})$, and there's an intermediate phase $\beta \in [\beta_{d}, \beta_{k}]$. In the glassy \& intermediate phase, MCMC / Langevin don't work (because of critical slowing down).
\begin{align}
	p(a|x) = e^{\beta \sum J_{ijk} a_i a_j a_k  - \gamma \sum_i (a_i -  x_i)^2} 
\end{align}
Let's inspect the phase diagram as we tune the pinning field $\gamma(\tau)$ (s.t. $\gamma(\tau = 0) = 0$ and $\gamma(\tau = \infty) = \infty$). 

What this suggests is that below $T_{crit}$, diffusion cannot sample well, while BP still can. (What Mezard means by this is that we can use BP to compute marginals / expectation values without sampling. This is NOT contradictory to UNets can realized BP algorithms because that paper talks about using BP for sampling).

\section{Curie Weiss Model}
Consider the target distribution over varaibles $a \in \{\pm 1\}^d$
\begin{align}
	p_0(a) & = \frac{1}{Z} e^{-\frac{\beta}{2d}(\sum_i a_i)^2} \\
	p_0(a)& = \frac{1}{Z} \int dm e^{- \frac{\beta d}{2} m^2 + \beta m \sum_i a_i} & \text{Introduced auxiliary variable } m
\end{align}
So now you have a joint distribution $p_0(m, a)$. If you then marginalize over $a$, you get
\begin{align}
	p_0(m) =\frac{1}{Z} e^{- \beta d [ \frac{m^2}{2} - \frac{1}{\beta} \log (\cosh(\beta m))]}
\end{align}
The free energy $F(m)$ has a minimum at $m = \tanh \beta m$ (self consistency equation). While this is nice, for the purposes of pedagody, let's look at a a slightly simpler model (modified Curie Weiss), which has the following distribution
\begin{align}
	p_0(a) = \frac{1}{2} \underbrace{\prod_i \frac{e^{\beta m^* a_i}}{2 \cosh(\beta m^*)}}_{\equiv p^+(a)} + \frac{1}{2} \underbrace{\prod_i \frac{e^{- \beta m^* a_i}}{2 \cosh (\beta m^*)}}_{\equiv p^-(a)}
\end{align} 
meaning you have $\mathbb P[m= \pm m^*] = 1/2$.

\subsection{Generative Diffusion w/ Perfect Score}
The probability distribution endowed via the OU process becomes
\begin{align}
	p_t(x,a) & = \frac{1}{2} p_t^+(x,a) + \frac{1}{2} p_t^-(x,a)\\
	\text{where } p_t^{\pm}(x,a) & = \prod_i \frac{e^{-(x^2_i - e^{-2t})/2\Delta_t}}{\sqrt{2\pi \Delta_t}^d} \prod_i \frac{e^{(\pm \beta m^* + x_i e^{-t}/\Delta_t)a_i}}{2 \cosh(\pm \beta m^* + x_i e^{-t} /\Delta_t)}
\end{align}
we can compute the score via Tweedy's Formula. 
\begin{align}
	p_t(a |x) = \frac{1}{Z} \left( \prod_i e^{(\beta m^* + x_i e^{-t}/\Delta_t) a_i}  + \prod_i e^{(-\beta m^* + x_i e^{-t}/\Delta_t) a_i} \right)
\end{align}
Now we can compute $\langle a\rangle_{x,t}$ (which takes two pages of calculation in Mezard's notes)
\begin{align}
	\langle a_i \rangle_{x,t} & = \frac{Q_+(x) \tanh(\beta m^* + x_i \frac{e^{-t}}{\Delta_t}) +  Q_-(x) \tanh(- \beta m^* + x_i \frac{e^{-t}}{\Delta_t})}{Q_+(x) + Q_-(x)}\\
	\text{s.t. } Q_{\pm}(x) & = \prod_j(1 \pm m^* \tanh (x_j e^{-t}/ \Delta_t))
\end{align}
The backwards langevin becomes
\begin{align}
	\frac{dx_i}{d\tau} & = x_i  + 2\varphi_i(x,\tau) + \eta_i(\tau)\\
	\text{s.t. }\varphi_i(x,\tau) & = - \frac{x_i}{\Delta_{t_g - \tau}} + \frac{e^{-(t_f -\tau)}}{\Delta_{t_f -\tau}} \langle a_i \rangle_{x, t_f-\tau}
\end{align}
\subsection{Speciation Transition}
\textbf{In the Regime ${t = t_{f} - \tau \gg 1}$} (so you're at the beginning of the backwards process). Then
\begin{align}
	Q_{\pm}(x) & \underset{t \gg 1}{\to} \exp\left(\pm m^* \frac{e^{-t}}{\Delta_t} \sum_j x_j \right) & \text{Linearized tanh(...)}
\end{align}
The Langevin equaiton in this limit becomes
\begin{align}
	\frac{dx_i}{d\tau} = x_i(1 - \frac{2}{\Delta_t} ) + 2 m^* \frac{e^{-t}}{\Delta_t} \tanh [m^* \frac{e^{-t}}{\Delta_t} \sum_j x_j] + \eta_i(t)
\end{align}
Each variable $x_i$, time-evolves \emph{almost} independently and is coupled via the order parameter (which ends up being the magnetization $m^*$)! We now introduce an order parameter $\mu$
\begin{align}
	\mu = \frac{1}{\sqrt{d}} \sum_{i=1}^d x_i
\end{align}
Now we can ask the question what is the time-evolution of $\mu$ just by summing $x_i$
\begin{align}
	\frac{d\mu}{d\tau} & = - \mu + 2 m^* \sqrt{d} e^{-t} \tanh(m^* \sqrt{d} e^{-t} \mu) + \eta(\tau)\\
	& =- \frac{\partial V_\tau(\mu)}{\partial \mu} + \eta(\tau)
\end{align}
where $V_\tau(\mu) = \frac{1}{2}\mu^2 - 2 \log \cosh(m^* \sqrt{d} e^{-t} \mu)$.
\begin{sidework}
	In the regime $\sqrt{d}e^{-t} \ll 1$, we see that $V_\tau(\mu) \simeq \mu^2$.\\
	\\
	In the regime $\sqrt{d} e^{-t} \gg 1$, you find $\log \cosh(...) \to  |\mu| m^* \sqrt{d} e^{-t}$, meaning $V_\tau(\mu) \simeq \mu^2  + |\mu|$ (it has symmetry breaking!).\\
	\\
	This means there's some characteristic time-scale $t_{sp}$ (which Mezard calls the speciation time), in which the samples must choose wether to the positive magnetization distribution or the negative magnetization distribution. \\
	\\
	In this particular example $t_{sp} = \frac{1}{2}\log d$
\end{sidework}
\begin{sidework}
	\textbf{Experimental measurements of speciation}
	\begin{itemize}
	\item Cloning (see Biroli, Bonnaic, De Bortali). Imagine you have one partical, and you run it through backwards Lagevin $\partial_t x_1 = x_i + 2 \varphi(x,t)$. Once it gets to a specific time, we clone it, and then ask "what is the probability that the two clones go to the same class?". Repeat for many different times, and hopefully you can experimentally measure hte speciation time.
	\item U-Turn (see Behjoo-Cherthov). Start at a point in the database, and then put it through the forward process and arrive at a specific time. Now you let it evolve backwards using Langevin, and ask the quesiton if it goes back to the same class or a new class. Repeat for many different times.
\end{itemize}
\end{sidework}
\textbf{In the Regime (II)} $t < t_{sp}$. \\
\\
Analysis reveals that $\frac{Q_-(x)}{Q_+(x)} \sim e^{- a d}$. This allows us to say
\begin{align}
	\langle a_i \rangle_{x,t} & = \tanh \left(\beta m^* + x_i \frac{e^{-t}}{\Delta_t} \right)\\
	\implies \frac{dx_i}{d\tau} & = x_i - 2 \frac{x_i}{\Delta_{t_f - \tau}} -2 \frac{e^{-(t_f-\tau)}}{\Delta_{t_f - \tau}} \tanh \left(\beta m^* + x_i \frac{e^{-t}}{\Delta_t} \right) + \eta(\tau)
\end{align}
So you can see the measure will concentrate from real-valued variables to a binary variables.

\subsubsection{asdf}
\begin{align}
	p_t(x) &  = \int da~ p_0(a) \frac{e^{-(x - ae^{-t})^2/2\Delta_t}}{\sqrt{2\pi \Delta_t}^d}\\
	& =  e^{-x^2/2\Delta_t}  e^{g(x)}
\end{align}
where $g(x) = \log \left [\int da ~ p_0(a) ~ e^{\frac{-1}{2} \frac{|a|^2 e^{-2t}}{\Delta_t} + \frac{e^{-t}}{\Delta_t} x\cdot a} \right]$. If we're interested in sampling high-dimensional distirbutions (that is we have a lot of spins) $d \gg 1$, this implies the speciation time will grow, so we'd like to inspect the limit $t \gg 1 \implies e^{-t} \ll 1$. Now we can do a "Landau-Type expansion of $g(x)$"
\begin{align}
	g(x) = \frac{e^{-t}}{\Delta_t} \sum_i x_i \langle a_i\rangle + \frac{1}{2} \frac{e^{-2t}}{\Delta_t} \sum_{ij} C_{ij} x_i x_j + \mathcal O(x^3)
\end{align}
where $C_{ij} = \langle a_i a_j \rangle_{p_0} - \langle a_i \rangle_{p_0} \langle a_j \rangle_{p_0}$. Plugging in this expansion back into $p_t(x)$, and then loging it, we get
\begin{align}
	\implies \log p_t(x) \approx c_0 + \sum_i x_i ... + \sum_{ij} \left ( - \frac{\delta_{ij}}{2 \Delta_t} + \frac{1}{2 \Delta_t} e^{-2t} C_{ij}\right) x_i x_j
\end{align}
Let's inspect the $\sum_{ij}$ term. This really looks like a matrix
\begin{align}
	M = \mathbb I - e^{-2t} C
\end{align}
so in $t \gg 1 \implies M \succ 0$. 

\section{Generalization vs Memorization in Diffusion Models}
Consider the dataset $\mathcal D = \{a^\mu\}_{\mu = 1, ... ,n}$, meaning you have an empirical distribution $p_0^{emp}(a) = \frac{1}{n} \sum_\mu \delta(a - a^\mu)$. This means your empirical distribution (the data noised by the OU process) becomes $p_t^{emp}(x) = \frac{1}{n} \sum_\mu \frac{e^{-(x - a^\mu e^{-t})^2/2\Delta_t}}{\sqrt{2\pi \Delta_t}^d}$. This means your empirical score is $\varphi(x,t) = \nabla_x \log p_t^{emp}(x)$. \\
\\
Recall that a perfectly train system, that is you find a score which perfectly matches $\varphi$, then $p_0(\tau =t_f) = p_0^{emp}$. So in training your model, you actually want to do this, this would be useless.
\begin{definition}
	[Collapse Time] Marc defines a heuristic model collapse time. Basically you perform your experiment for $t_{spec}$ time, but instead of class, you see if it goes back to the same training data point.
\end{definition}
\subsection{Rigorous Calculation}
Up until now, we have just been talking about heuristics. Here's let's attempt to do a rigorous calculation to find a collapse time.\\
\\
First let's define a quantity which is not-extensive w.r.t. dimension.
\begin{definition}
	[Entropy Criterion] 
	\begin{align}
		S(t) = - \frac{1}{d} \int dx ~ p_t(x) \log p_t(x)
	\end{align}
\end{definition}
Our goal will be compare this to $n$ separated Gaussians
\begin{align}
	S^{sep}(t) & = \frac{\log n}{d} + \frac{1}{2} + \frac{1}{2} \log (2\pi \Delta_t) & \text{Separated Gaussians}
\end{align}
we'll denote the difference as the \textbf{excess entropy}
\begin{align}
	f(t) = S^{sep}(t) - S(t)
\end{align}
In the limit $t \gg 1$
\begin{align}
	S(t) \to \frac{1}{2}\log (2\pi e)\\
	S^{sep}(t) \to \frac{\log n}{d} + \frac{1}{2} \log (2\pi e)
\end{align}
So to get any excess entropy, we'd need $\frac{\log n}{d} \sim \mathcal O(1)$, which means $n \sim \mathcal O(e^d)$ (the number of samples must be exponential in the number of dimensions / spins).
\\
\\
Now let's setup our problem. Consider getting $n/2$ samples from $\mathcal N(m, \sigma^2 \mathbb I)$ and $n/2$ samples from $\mathcal N(-m, \sigma^2 \mathbb I)$. I noise these points using OU process
\begin{align}
	p_t^{emp}(x) = \frac{1}{n} \sum_\mu \frac{e^{-(x-a^\mu e^{-t})^2/2\Delta_t}}{\sqrt{2\pi \Delta_t}^d}
\end{align}
Let's inspect the collapse time on going to training data point $a^1$. At time $t$, your noised trajectory is $x = a^1 e^{-t} + z \sqrt{\Delta_t}$. So we can decompose our model as
\begin{align}
	p_t(x) = \frac{1}{n\sqrt{2\pi \Delta_t}^d} (z_1 + z_{2, ..., n})
\end{align}
where $z_1 = e^{-(x - a^1 e^{-t})^2/2\Delta_t}$ and $z_{2, ..., n} = \sum_{\mu=2}^{n/2} e^{-(x - a^\mu e^{-t})^2/2\Delta_t} + \sum_{\mu = n/2 + 1}^n e^{-(x - a^\mu e^{-t})^2/2\Delta_t}$. The decomposition of $z_{2...n}$ is quite big so let's denote $\mathcal Z_+ \equiv \sum_{\mu=2}^{n/2} e^{-(x - a^\mu e^{-t})^2/2\Delta_t}$ and $\mathcal Z_{-} = \sum_{\mu = n/2 + 1}^n e^{-(x - a^\mu e^{-t})^2/2\Delta_t}$.\\
\\
In the large $d$, large $n$ limit, 
\begin{align}
	\mathcal Z_{\pm} & \sim e^{-d \psi_{\pm}}\\
	z_1 & = e^{- |z|^2/2} \simeq e^{-d/2}
\end{align}
which $\psi_{\pm}$ is the large deviation free entropy.
\begin{align}
	p_t^{emp} \propto (z_1 + e^{-d \psi_+} + e^{-d \psi_-})
\end{align}
Cases
\begin{itemize}
	\item If $\psi_+ < \frac{1}{2}$, this means $z_1$ is negligable. Meaning you can pick out other data points. Therefore the model has NOT collapsed.
	\item If $\psi_+ > \frac{1}{2}$, this means other terms are exponentially supressed w.r.t. $z_1$. Therefore you'll only recover $z_1$, hence the model has collapsed.
\end{itemize}
So what is $\psi_{\pm}$?
\begin{align}
	\mathcal Z_+ & = \sum_{\mu=2}^{e^{\alpha d /2}} e^{-d \epsilon^\mu}, ~~ \epsilon^\mu = \frac{(x - a^\mu e^{-t})^2}{2 \Delta_t d}\\
	\psi_+ & = - \frac{1}{d} \log \mathcal Z_+
\end{align}
It's the free energy of the \textbf{Random Energy Model}. Marc said he's tired of teaching Replica Trick calculations hahaha, so he'll solve it with formal methods using large deviation theory.\\
\\
Fix $x = a^1 e^{-t} + z \sqrt{\Delta_t}$. If we assume $a^\mu$ are sampled iid, then this means $\epsilon^\mu$ are also iid. It turns out the probability distribution of $\epsilon$ becomes
\begin{align}
	p(\epsilon) \simeq_{d \to \infty} e^{d s(\epsilon)}
\end{align}
where $s(\epsilon)$ a concave function with a maximum $s(\epsilon_0) = 0$
\begin{align}
	\epsilon^\mu \sim \frac{1}{2d \Delta_t}(|x|^2 - |a^\mu|^2 e^{-2t})
\end{align}
However because our partition function has exponentially points, you can't just estimate the typical point.
\begin{align}
	\int d\epsilon ~p(\epsilon) e^{- \lambda d \epsilon} & = \int d\epsilon ~e^{d(s(\epsilon - \lambda \epsilon)}\\
	\mathbb E_a[e^{- \lambda |x- ae^{-t}|^2/2\Delta_t}& = e^{d g(\lambda)}
\end{align}
where $g(\lambda) = \max_\epsilon (s(\epsilon) - \lambda \epsilon)$, which you find via inverse Legendre transform, which you see is $g(\lambda) = s(\epsilon)$.
\begin{align}
	\mathcal Z_+ = \frac{1}{2}e^{\alpha d} \int d\epsilon e^{d(s(\epsilon) -\epsilon)}
\end{align}
And you find the collapse time $t_{collapse} = \frac{1}{2} \log (1 + \frac{\sigma^2}{e^{2\alpha} - 1})$, where $e^{2\alpha} = n^{2/d}$. If you redo this calculation on a hidden manifold, $e^{2\alpha} \to e^{2/\text{dim(manifold)}}$.


















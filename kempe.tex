Kempe's talk is titled "Synthetic Data: The Good, The Bad, The Ugly (and The Math)". I like the data.\\
\\
The premise is we are at the point where a fair amount of data on the internet has been created via AI. And we all know about model collapse (that is if you train an LLM on data generated by an ML program will eventually lead to models performing horribly). This is not so surprising because if you sample a gaussian, and fit it, then resample, on and on--- you'll find the Gaussian's variance will go down--- regression to the mean. So how can we model \& understand this phenomena!

So some causes
\begin{enumerate}
	\item Finite sample error
	\item Expressitivity error (model under capacity)
	\item Approximation error (due to optimization bias, etc.)
	\item Inteference error.
\end{enumerate}
Kempe likes to study these from the perspective of scaling laws.
\subsection{Infinite Memory Model}
This was published by Hutler in 2022.\\
\\
Consider a dataset $\mathcal D_T = \{(i_t, f(i_t))\}_{t=1}^T \subset \mathcal D$. And we have a memory model
\begin{align}
	\mathcal A(i, \mathcal D_T) = \begin{cases}
		f(i)  &  \text{if } i = \i e \text{ s.t. } \exists 1 \leq e \leq T\\
		\perp &  \text{otherwise}
	\end{cases}
\end{align}
what we notice is that $p(i) \sim i^{-\beta}$. 
There's a test error 
\begin{align}
	E_{test}(T)&  = \mathbb P(i \neq ie : i \leq \ell \leq T)) \\
	& = \sum_{i=1}^\infty p(i) (1 - p(i))^T\\
	& \simeq \sum_{i=1}^\infty i^{-\beta  } (1- i^{-\beta})^T \simeq \sum i^{-\beta e^{-(i^{-\beta})T}}
\end{align}
Let $c = 1 -1/\beta$. Then
\begin{align}
	= T^c \sum_{i} i^{-\beta} e^{-(i^{-\beta}) T}  \asymp  \Gamma(c, T k^{-\beta}) - \Gamma (c, T) = \mathcal O(1)
\end{align}
Ok... Now let's see the behavior as we retrain based off of the previous model. That is our dataset will now be $\mathcal D_T = \{(i, \mathcal A_{T_0}(i))\}_{i}$, so $\mathcal A_{T_0}(i)$ is like ChatGPT trained on very clean internet data, and this current dataset is the internet 1 year after ChatGPT.

Notice that $i$ appears $T_0 i^{-\beta}$ times in $\mathcal D_{T_0}$. So in the limit $T_0 i^{-\beta} \ll 1$. Then if $i > k$, $\mathcal A_{T_0}$ hasn't seen it.
\begin{align}
	\mathcal A'_{T_0, T}(i) = \begin{cases}
		f(i) & \text{if } i \leq k \text{ and } i=ie ~ 1 \leq \ell \leq T\\
		\perp & \text{otherwise}
	\end{cases}
\end{align}
The test error now chnage 
\begin{align}
	E_{test} = \mathbb P (i > k : 1 \leq \ell \leq T) = \sum_{i=1}^k p(i) (1-p(i))^T + \sum_{i > k}p(i) \asymp k^{-(p-1)}= k^{-c\beta}
\end{align}
If we redo this calculation in the limit $1 \ll T < k^\beta$. Then $\Gamma(c, Tk^{-\beta}) \sim \mathcal O(1)$, so 
\begin{align}
	E_{test} \asymp k^{-\beta c} + T^{-c} \asymp T^C
\end{align}

Another case $T \leq k^2$, then
\begin{align}
	E_{test} \asymp k^{-\beta c} + T^{-C} \asymp k^{-\beta c}
\end{align}
Basically, you redo this calculation over and over, and we see this scaling law apppear up everywhere. SO the test error is limited by these bounds.

\subsection{Kernel and Regression}
Consider your inputs $x \sim \mathcal N(0, \Sigma) \in \mathbb R^D$ and they're noisy $\epsilon \sim \mathcal N(0, \sigma^2) \in \mathbb R$ and the label $y = x^T W_0 + \epsilon$. Lets' start by assumping $d < T$, we have less data than parameters.
\begin{align}
	E_{test} = \mathbb E_{x, \epsilon} || x^T \hat w - y ||^2 - \sigma^2
\end{align}
The best fit $\hat w$ is via oridnary least squares between $X$ and $Y$. Meaning
\begin{align}
	\hat w & = (X_0^T  X_0)^{-1} X_0^T Y_0\\
	&  = (X_0^T X_0)^{-1} X_0^T (X_0 w_0 + E_0) & Y_0 = X_0 w_0 + E_0 \text{ (Original fit)}\\
	& = w_0 + X_0^T E_0
\end{align}
With this model, the test error we get is
\begin{align}
	E_{tot} & = ||(\hat w - w_0)||^2 \\
	& = \mathbb E_\epsilon || X_0^T E_0 ||^2\\
	& = \sigma^2 \mathbb E ~\text{Tr}(X_0(X_0^T X_0)^{-2} X_0^T )\\
	& = \sigma^2 \mathbb E ~ \text{Tr}(X_0^T X_0)^{-1}
\end{align}
Recall that $X_0 \sim \mathcal N(0, \Sigma)$, we can use some random matrix theory to compute this
\begin{align}
	& =  \sigma^2 \frac{d}{T-d-1} \sim \frac{\sigma^2 d}{T}
\end{align}
\subsubsection{Model Collapse}
So we did this just on one weight. So what happens if we do this recurisvely, over and over. Let's look at how the test error at the $i$'th iteration changes depending on the ground truth $w_0$
\begin{align}
	E_{test}^{(i)} & = || \hat w_i - w_0 ||_2\\
\end{align}
Let's start at $i=2$
\begin{align}
	E_{test}^{(2)} & = || \hat w_2 - w_0 ||_2 = || \underbrace{\hat w_2 - \hat w_1}_{X_1^T E_1}  + \underbrace{\hat w_1 - w_0}_{X_0^T E_0} ||^2\\
	& \asymp \frac{d \epsilon^2 }{T} + \frac{d \epsilon^2}{T_0}
\end{align}
Ok if we do this $n$ times
\begin{align}
	E_{test}^{(n)} = \frac{d \epsilon^2}{T} + \frac{n d \epsilon^2}{T_0}
\end{align}













